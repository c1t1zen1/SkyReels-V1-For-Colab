{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [SkyReels](https://www.skyreels.ai/) V1: Human-Centric Video Foundation Model\n",
        "The first and most advanced open-source human-centric video foundation model.\n",
        "\n",
        "By fine-tuning HunyuanVideo on 10 Million high-quality film and television clips.\n",
        "\n",
        "This file is [SkyReels For Colab](https://github.com/c1t1zen1/SkyReels-V1-For-Colab) - c1t1zen1 version\n",
        "\n",
        "## Start by selecting A100 GPU Runtime.\n",
        "Works on L4 GPU also but much slower.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Text-To-Video and Image-To-Video available.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Step 1 - Clones the repo and installs requirements.\n",
        "\n",
        "Step 2 v1 - Loads the weights and starts a Gradio app.\n",
        "  - click on the URL link to open Gradio in a new browser window.\n",
        "\n",
        "Step2 v2 - Gradio with extended settings.\n",
        "\n",
        "Step 2 v3 - Gradio with Cluade API prompt enhance built in.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HjCBJW_-RVjp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5b466ba8"
      },
      "outputs": [],
      "source": [
        "# @title Step 1 - Clone Repo and Install Requirements\n",
        "# Clone the repository\n",
        "!git clone https://github.com/SkyworkAI/SkyReels-V1.git &> /dev/null\n",
        "# !git clone https://github.com/c1t1zen1/SkyReels-V1-For-Colab.git &> /dev/null\n",
        "\n",
        "# Install required packages (add any others as needed)\n",
        "!pip install gradio -q\n",
        "\n",
        "# Append the cloned repo to sys.path so that its modules can be imported\n",
        "import sys\n",
        "sys.path.append('SkyReels-V1')\n",
        "%cd SkyReels-V1\n",
        "!pip install -r requirements.txt -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2 - Load Skyreels Weights and Start Gradio\n",
        "# @markdown -  Select t2v for Text-To-Video or i2v for Image-To-Video Setup\n",
        "#WORKING CODE\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "import time\n",
        "import gradio as gr\n",
        "from diffusers.utils import export_to_video, load_image\n",
        "\n",
        "from skyreelsinfer import TaskType\n",
        "from skyreelsinfer.skyreels_video_infer import SkyReelsVideoSingleGpuInfer\n",
        "from skyreelsinfer.offload import OffloadConfig\n",
        "\n",
        "def init_process_group():\n",
        "    \"\"\"Initialize the process group only if it hasn't been initialized yet\"\"\"\n",
        "    if not dist.is_initialized():\n",
        "        # Set environment variables\n",
        "        os.environ['MASTER_ADDR'] = 'localhost'\n",
        "        os.environ['MASTER_PORT'] = '23456'\n",
        "        os.environ['WORLD_SIZE'] = '1'\n",
        "        os.environ['RANK'] = '0'\n",
        "        os.environ['LOCAL_RANK'] = '0'\n",
        "\n",
        "        # Initialize the process group\n",
        "        dist.init_process_group(\n",
        "            backend=\"nccl\",\n",
        "            init_method=\"tcp://127.0.0.1:23456\",\n",
        "            world_size=1,\n",
        "            rank=0\n",
        "        )\n",
        "\n",
        "class SimpleSkyReelsInfer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        task_type: TaskType,\n",
        "        model_id: str,\n",
        "        quant_model: bool = True,\n",
        "    ):\n",
        "        # Create proper offload config\n",
        "        offload_config = OffloadConfig(\n",
        "            high_cpu_memory=True,\n",
        "            parameters_level=True,\n",
        "            compiler_transformer=False,\n",
        "            compiler_cache=\"./compiler_cache\"\n",
        "        )\n",
        "\n",
        "        # Monkey patch the init_process_group call in SkyReelsVideoSingleGpuInfer\n",
        "        def dummy_init(*args, **kwargs):\n",
        "            pass\n",
        "\n",
        "        # Store the original function\n",
        "        original_init = dist.init_process_group\n",
        "\n",
        "        try:\n",
        "            # Replace the init function temporarily\n",
        "            dist.init_process_group = dummy_init\n",
        "\n",
        "            # Create the pipe instance\n",
        "            self.pipe = SkyReelsVideoSingleGpuInfer(\n",
        "                task_type=task_type,\n",
        "                model_id=model_id,\n",
        "                quant_model=quant_model,\n",
        "                local_rank=0,\n",
        "                world_size=1,\n",
        "                is_offload=True,\n",
        "                offload_config=offload_config,\n",
        "                enable_cfg_parallel=False\n",
        "            )\n",
        "        finally:\n",
        "            # Restore the original function\n",
        "            dist.init_process_group = original_init\n",
        "\n",
        "    def inference(self, kwargs):\n",
        "        if not isinstance(kwargs, dict):\n",
        "            raise ValueError(\"kwargs must be a dictionary\")\n",
        "\n",
        "        print(f\"Inference kwargs: {kwargs}\")  # Debug print\n",
        "\n",
        "        if \"seed\" in kwargs:\n",
        "            kwargs[\"generator\"] = torch.Generator(\"cuda\").manual_seed(kwargs[\"seed\"])\n",
        "            del kwargs[\"seed\"]\n",
        "\n",
        "        return self.pipe.pipe(**kwargs).frames[0]\n",
        "\n",
        "def generate_video(prompt, seed, image=None):\n",
        "    global task_type, predictor\n",
        "    print(f\"image: {type(image)}\")\n",
        "    print(f\"prompt: {prompt}\")\n",
        "    print(f\"seed: {seed}\")\n",
        "\n",
        "    if seed == -1:\n",
        "        random.seed(time.time())\n",
        "        seed = int(random.randrange(4294967294))\n",
        "\n",
        "    kwargs = {\n",
        "        \"prompt\": prompt,\n",
        "        \"height\": 544,\n",
        "        \"width\": 960,\n",
        "        \"num_frames\": 97,\n",
        "        \"num_inference_steps\": 30,\n",
        "        \"seed\": seed,\n",
        "        \"guidance_scale\": 6.0,\n",
        "        \"embedded_guidance_scale\": 1.0,\n",
        "        \"negative_prompt\": \"Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion\",\n",
        "        \"cfg_for\": False\n",
        "    }\n",
        "\n",
        "    if task_type == \"i2v\":\n",
        "        assert image is not None, \"please input image\"\n",
        "        if isinstance(image, str):\n",
        "            image = Image.open(image)\n",
        "        kwargs[\"image\"] = image\n",
        "\n",
        "    print(f\"Final kwargs: {kwargs}\")  # Debug print\n",
        "\n",
        "    try:\n",
        "        output = predictor.inference(kwargs)\n",
        "        save_dir = f\"./result/{task_type}\"\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        video_out_file = f\"{save_dir}/{prompt[:100].replace('/','')}_{seed}.mp4\"\n",
        "        print(f\"generate video, local path: {video_out_file}\")\n",
        "        export_to_video(output, video_out_file, fps=24)\n",
        "        return video_out_file, kwargs\n",
        "    except Exception as e:\n",
        "        print(f\"Error during inference: {str(e)}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        raise\n",
        "\n",
        "def init_predictor(task_type: str):\n",
        "    global predictor\n",
        "    try:\n",
        "        # Set CUDA device first\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.init()\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.set_device(0)\n",
        "\n",
        "        # Initialize process group first\n",
        "        init_process_group()\n",
        "\n",
        "        model_id = \"Skywork/SkyReels-V1-Hunyuan-I2V\" if task_type == \"i2v\" else \"Skywork/SkyReels-V1-Hunyuan-T2V\"\n",
        "\n",
        "        predictor = SimpleSkyReelsInfer(\n",
        "            task_type=TaskType.I2V if task_type == \"i2v\" else TaskType.T2V,\n",
        "            model_id=model_id,\n",
        "            quant_model=True\n",
        "        )\n",
        "        print(\"Predictor initialized successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Initialization error: {str(e)}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        raise\n",
        "\n",
        "def create_gradio_interface(task_type):\n",
        "    \"\"\"Create a Gradio interface based on the task type.\"\"\"\n",
        "    if task_type == \"i2v\":\n",
        "        with gr.Blocks() as demo:\n",
        "            with gr.Row():\n",
        "                image = gr.Image(label=\"Upload Image\", type=\"filepath\")\n",
        "                prompt = gr.Textbox(label=\"Input Prompt\")\n",
        "                seed = gr.Number(label=\"Random Seed\", value=-1)\n",
        "            submit_button = gr.Button(\"Generate Video\")\n",
        "            output_video = gr.Video(label=\"Generated Video\")\n",
        "            output_params = gr.Textbox(label=\"Output Parameters\")\n",
        "\n",
        "            # Button logic\n",
        "            submit_button.click(\n",
        "                fn=generate_video,\n",
        "                inputs=[prompt, seed, image],\n",
        "                outputs=[output_video, output_params]\n",
        "            )\n",
        "    elif task_type == \"t2v\":\n",
        "        with gr.Blocks() as demo:\n",
        "            with gr.Row():\n",
        "                prompt = gr.Textbox(label=\"Input Prompt\")\n",
        "                seed = gr.Number(label=\"Random Seed\", value=-1)\n",
        "            submit_button = gr.Button(\"Generate Video\")\n",
        "            output_video = gr.Video(label=\"Generated Video\")\n",
        "            output_params = gr.Textbox(label=\"Output Parameters\")\n",
        "\n",
        "            # Button logic\n",
        "            submit_button.click(\n",
        "                fn=generate_video,\n",
        "                inputs=[prompt, seed],\n",
        "                outputs=[output_video, output_params]\n",
        "            )\n",
        "    return demo\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set environmental variables\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "    # Print system info\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
        "\n",
        "    task_type = \"i2v\" # @param [\"i2v\",\"t2v\"]\n",
        "    print(\"Starting initialization...\")\n",
        "    init_predictor(task_type)\n",
        "\n",
        "    # Create and launch the Gradio interface\n",
        "    demo = create_gradio_interface(task_type)\n",
        "    demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BatFEzSuHeVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2 v2 - Load Skyreels Weights and Start Extended Gradio\n",
        "# @markdown -  Select t2v for Text-To-Video or i2v for Image-To-Video Setup\n",
        "#WORKING CODE\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "import time\n",
        "import gradio as gr\n",
        "from diffusers.utils import export_to_video, load_image\n",
        "\n",
        "from skyreelsinfer import TaskType\n",
        "from skyreelsinfer.skyreels_video_infer import SkyReelsVideoSingleGpuInfer\n",
        "from skyreelsinfer.offload import OffloadConfig\n",
        "\n",
        "def init_process_group():\n",
        "    \"\"\"Initialize the process group only if it hasn't been initialized yet\"\"\"\n",
        "    if not dist.is_initialized():\n",
        "        # Set environment variables\n",
        "        os.environ['MASTER_ADDR'] = 'localhost'\n",
        "        os.environ['MASTER_PORT'] = '23456'\n",
        "        os.environ['WORLD_SIZE'] = '1'\n",
        "        os.environ['RANK'] = '0'\n",
        "        os.environ['LOCAL_RANK'] = '0'\n",
        "\n",
        "        # Initialize the process group\n",
        "        dist.init_process_group(\n",
        "            backend=\"nccl\",\n",
        "            init_method=\"tcp://127.0.0.1:23456\",\n",
        "            world_size=1,\n",
        "            rank=0\n",
        "        )\n",
        "\n",
        "class SimpleSkyReelsInfer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        task_type: TaskType,\n",
        "        model_id: str,\n",
        "        quant_model: bool = True,\n",
        "    ):\n",
        "        # Create proper offload config\n",
        "        offload_config = OffloadConfig(\n",
        "            high_cpu_memory=True,\n",
        "            parameters_level=True,\n",
        "            compiler_transformer=False,\n",
        "            compiler_cache=\"./compiler_cache\"\n",
        "        )\n",
        "\n",
        "        # Monkey patch the init_process_group call in SkyReelsVideoSingleGpuInfer\n",
        "        def dummy_init(*args, **kwargs):\n",
        "            pass\n",
        "\n",
        "        # Store the original function\n",
        "        original_init = dist.init_process_group\n",
        "\n",
        "        try:\n",
        "            # Replace the init function temporarily\n",
        "            dist.init_process_group = dummy_init\n",
        "\n",
        "            # Create the pipe instance\n",
        "            self.pipe = SkyReelsVideoSingleGpuInfer(\n",
        "                task_type=task_type,\n",
        "                model_id=model_id,\n",
        "                quant_model=quant_model,\n",
        "                local_rank=0,\n",
        "                world_size=1,\n",
        "                is_offload=True,\n",
        "                offload_config=offload_config,\n",
        "                enable_cfg_parallel=False\n",
        "            )\n",
        "        finally:\n",
        "            # Restore the original function\n",
        "            dist.init_process_group = original_init\n",
        "\n",
        "    def inference(self, kwargs):\n",
        "        if not isinstance(kwargs, dict):\n",
        "            raise ValueError(\"kwargs must be a dictionary\")\n",
        "\n",
        "        print(f\"Inference kwargs: {kwargs}\")  # Debug print\n",
        "\n",
        "        if \"seed\" in kwargs:\n",
        "            kwargs[\"generator\"] = torch.Generator(\"cuda\").manual_seed(kwargs[\"seed\"])\n",
        "            del kwargs[\"seed\"]\n",
        "\n",
        "        return self.pipe.pipe(**kwargs).frames[0]\n",
        "\n",
        "def round_to_multiple_of_16(n):\n",
        "    \"\"\"Round a number to the nearest multiple of 16.\"\"\"\n",
        "    return ((n + 8) // 16) * 16\n",
        "\n",
        "def generate_video(prompt, seed, negative_prompt, width, height, num_frames,\n",
        "                   num_inference_steps, guidance_scale, embedded_guidance_scale, image=None):\n",
        "    global task_type, predictor\n",
        "\n",
        "    # Debug prints\n",
        "    print(f\"Starting video generation with parameters:\")\n",
        "    print(f\"- Image type: {type(image)}\")\n",
        "    print(f\"- Prompt: {prompt}\")\n",
        "    print(f\"- Original dimensions: {width}x{height}\")\n",
        "\n",
        "    # Round dimensions to nearest multiple of 16\n",
        "    width = round_to_multiple_of_16(int(width))\n",
        "    height = round_to_multiple_of_16(int(height))\n",
        "    print(f\"- Adjusted dimensions: {width}x{height}\")\n",
        "\n",
        "    print(f\"- Frames: {num_frames}\")\n",
        "    print(f\"- Steps: {num_inference_steps}\")\n",
        "    print(f\"- Seed: {seed}\")\n",
        "\n",
        "    # Handle random seed\n",
        "    if seed == -1:\n",
        "        random.seed(time.time())\n",
        "        seed = int(random.randrange(4294967294))\n",
        "\n",
        "    # Build kwargs dictionary\n",
        "    kwargs = {\n",
        "        \"prompt\": prompt,\n",
        "        \"height\": height,  # Now guaranteed to be divisible by 16\n",
        "        \"width\": width,   # Now guaranteed to be divisible by 16\n",
        "        \"num_frames\": int(num_frames),\n",
        "        \"num_inference_steps\": int(num_inference_steps),\n",
        "        \"seed\": seed,\n",
        "        \"guidance_scale\": float(guidance_scale),\n",
        "        \"embedded_guidance_scale\": float(embedded_guidance_scale),\n",
        "        \"negative_prompt\": negative_prompt,\n",
        "        \"cfg_for\": False\n",
        "    }\n",
        "\n",
        "    # Handle image for i2v mode\n",
        "    if task_type == \"i2v\":\n",
        "        if image is None:\n",
        "            raise ValueError(\"Image is required for image-to-video mode\")\n",
        "\n",
        "        if isinstance(image, str):\n",
        "            try:\n",
        "                image = Image.open(image)\n",
        "                # Resize image if needed while maintaining aspect ratio\n",
        "                aspect_ratio = image.width / image.height\n",
        "                new_width = width\n",
        "                new_height = int(width / aspect_ratio)\n",
        "                if new_height > height:\n",
        "                    new_height = height\n",
        "                    new_width = int(height * aspect_ratio)\n",
        "                # Round the dimensions to multiples of 16\n",
        "                new_width = round_to_multiple_of_16(new_width)\n",
        "                new_height = round_to_multiple_of_16(new_height)\n",
        "                image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"Error processing image: {str(e)}\")\n",
        "\n",
        "        kwargs[\"image\"] = image\n",
        "\n",
        "    print(f\"Final generation parameters: {kwargs}\")\n",
        "\n",
        "    try:\n",
        "        # Generate the video\n",
        "        output = predictor.inference(kwargs)\n",
        "\n",
        "        # Save the video\n",
        "        save_dir = f\"./result/{task_type}\"\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # Create a more informative filename\n",
        "        base_filename = f\"{prompt[:50].replace(' ', '_').replace('/','')}_{width}x{height}_{num_frames}fr_{seed}\"\n",
        "        video_out_file = f\"{save_dir}/{base_filename}.mp4\"\n",
        "\n",
        "        print(f\"Saving video to: {video_out_file}\")\n",
        "        export_to_video(output, video_out_file, fps=24)\n",
        "\n",
        "        # Return both the video file and the parameters used\n",
        "        return video_out_file, kwargs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during video generation: {str(e)}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        raise ValueError(f\"Video generation failed: {str(e)}\")\n",
        "\n",
        "def create_gradio_interface(task_type):\n",
        "    \"\"\"Create an enhanced Gradio interface with more controls.\"\"\"\n",
        "\n",
        "    css = \"\"\"\n",
        "        .container { margin: 15px; }\n",
        "        .output-panel { margin-top: 20px; }\n",
        "        .control-panel { padding: 10px; border: 1px solid #ccc; border-radius: 8px; }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(css=css) as demo:\n",
        "        gr.Markdown(\"# SkyReels Video Generation\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### Input Controls\")\n",
        "                with gr.Group(elem_classes=\"control-panel\"):\n",
        "                    # Image upload for i2v mode\n",
        "                    if task_type == \"i2v\":\n",
        "                        image = gr.Image(\n",
        "                            label=\"Upload Image\",\n",
        "                            type=\"filepath\",\n",
        "                            # tool=\"editor\",\n",
        "                            elem_id=\"input-image\"\n",
        "                        )\n",
        "\n",
        "                    # Basic parameters\n",
        "                    prompt = gr.Textbox(\n",
        "                        label=\"Input Prompt\",\n",
        "                        placeholder=\"Describe what you want to generate...\",\n",
        "                        lines=3\n",
        "                    )\n",
        "                    negative_prompt = gr.Textbox(\n",
        "                        label=\"Negative Prompt\",\n",
        "                        value=\"Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion\",\n",
        "                        lines=2\n",
        "                    )\n",
        "\n",
        "                    with gr.Accordion(\"Advanced Settings\", open=False):\n",
        "                        seed = gr.Number(\n",
        "                            label=\"Random Seed (-1 for random)\",\n",
        "                            value=-1,\n",
        "                            minimum=-1,\n",
        "                            precision=0\n",
        "                        )\n",
        "                        width = gr.Slider(\n",
        "                            label=\"Width\",\n",
        "                            minimum=128,\n",
        "                            maximum=1024,\n",
        "                            value=512,\n",
        "                            step=16  # Changed to 16 to ensure valid values\n",
        "                        )\n",
        "                        height = gr.Slider(\n",
        "                            label=\"Height\",\n",
        "                            minimum=128,\n",
        "                            maximum=1024,\n",
        "                            value=512,\n",
        "                            step=16  # Changed to 16 to ensure valid values\n",
        "                        )\n",
        "                        num_frames = gr.Slider(\n",
        "                            label=\"Number of Frames\",\n",
        "                            minimum=16,\n",
        "                            maximum=128,\n",
        "                            value=97,\n",
        "                            step=1\n",
        "                        )\n",
        "                        num_inference_steps = gr.Slider(\n",
        "                            label=\"Inference Steps\",\n",
        "                            minimum=1,\n",
        "                            maximum=100,\n",
        "                            value=30,\n",
        "                            step=1\n",
        "                        )\n",
        "                        guidance_scale = gr.Slider(\n",
        "                            label=\"Guidance Scale\",\n",
        "                            minimum=1,\n",
        "                            maximum=20,\n",
        "                            value=6.0,\n",
        "                            step=0.5\n",
        "                        )\n",
        "                        embedded_guidance_scale = gr.Slider(\n",
        "                            label=\"Embedded Guidance Scale\",\n",
        "                            minimum=0.1,\n",
        "                            maximum=5.0,\n",
        "                            value=1.0,\n",
        "                            step=0.1\n",
        "                        )\n",
        "\n",
        "                    submit_button = gr.Button(\n",
        "                        \"Generate Video\",\n",
        "                        variant=\"primary\",\n",
        "                        size=\"lg\"\n",
        "                    )\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### Output\")\n",
        "                with gr.Group(elem_classes=\"output-panel\"):\n",
        "                    output_video = gr.Video(label=\"Generated Video\")\n",
        "                    output_params = gr.JSON(label=\"Generation Parameters\")\n",
        "\n",
        "        # Event handlers\n",
        "        input_components = [\n",
        "            prompt,\n",
        "            seed,\n",
        "            negative_prompt,\n",
        "            width,\n",
        "            height,\n",
        "            num_frames,\n",
        "            num_inference_steps,\n",
        "            guidance_scale,\n",
        "            embedded_guidance_scale\n",
        "        ]\n",
        "\n",
        "        if task_type == \"i2v\":\n",
        "            input_components.append(image)\n",
        "\n",
        "        submit_button.click(\n",
        "            fn=generate_video,\n",
        "            inputs=input_components,\n",
        "            outputs=[output_video, output_params]\n",
        "        )\n",
        "\n",
        "        # Add help section\n",
        "        with gr.Accordion(\"Help\", open=False):\n",
        "            gr.Markdown(\"\"\"\n",
        "                ### Tips for best results:\n",
        "                - Use clear, descriptive prompts\n",
        "                - Experiment with different guidance scales\n",
        "                - Try different seeds for variation\n",
        "                - Adjust frame count based on your needs\n",
        "\n",
        "                ### Common issues:\n",
        "                - If generation fails, try reducing the resolution\n",
        "                - High guidance scales may produce stronger but potentially less stable results\n",
        "                - Larger frame counts will take longer to generate\n",
        "            \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set environmental variables\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "    # Print system info\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
        "\n",
        "    task_type = \"t2v\" # @param [\"i2v\",\"t2v\"]\n",
        "    print(\"Starting initialization...\")\n",
        "    init_predictor(task_type)\n",
        "\n",
        "    # Create and launch the Gradio interface\n",
        "    demo = create_gradio_interface(task_type)\n",
        "    demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "raiwLNhF6WEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2 v3 - Enhance Prompts with Claude API\n",
        "\n",
        "!pip install anthropic tiktoken -q\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "import time\n",
        "import gradio as gr\n",
        "from diffusers.utils import export_to_video, load_image\n",
        "import anthropic\n",
        "import json\n",
        "from functools import partial\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "from skyreelsinfer import TaskType\n",
        "from skyreelsinfer.skyreels_video_infer import SkyReelsVideoSingleGpuInfer\n",
        "from skyreelsinfer.offload import OffloadConfig\n",
        "\n",
        "# Initialize the Claude client\n",
        "api_key = \"\" # @param {\"type\":\"string\",\"placeholder\":\"Your Claude API Key Here - Optional\"}\n",
        "claude = anthropic.Anthropic(api_key=api_key)\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "def init_process_group():\n",
        "    \"\"\"Initialize the process group only if it hasn't been initialized yet\"\"\"\n",
        "    if not dist.is_initialized():\n",
        "        # Set environment variables\n",
        "        os.environ['MASTER_ADDR'] = 'localhost'\n",
        "        os.environ['MASTER_PORT'] = '23456'\n",
        "        os.environ['WORLD_SIZE'] = '1'\n",
        "        os.environ['RANK'] = '0'\n",
        "        os.environ['LOCAL_RANK'] = '0'\n",
        "\n",
        "        # Initialize the process group\n",
        "        dist.init_process_group(\n",
        "            backend=\"nccl\",\n",
        "            init_method=\"tcp://127.0.0.1:23456\",\n",
        "            world_size=1,\n",
        "            rank=0\n",
        "        )\n",
        "\n",
        "class SimpleSkyReelsInfer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        task_type: TaskType,\n",
        "        model_id: str,\n",
        "        quant_model: bool = True,\n",
        "    ):\n",
        "        # Create proper offload config\n",
        "        offload_config = OffloadConfig(\n",
        "            high_cpu_memory=True,\n",
        "            parameters_level=True,\n",
        "            compiler_transformer=False,\n",
        "            compiler_cache=\"./compiler_cache\"\n",
        "        )\n",
        "\n",
        "        # Monkey patch the init_process_group call in SkyReelsVideoSingleGpuInfer\n",
        "        def dummy_init(*args, **kwargs):\n",
        "            pass\n",
        "\n",
        "        # Store the original function\n",
        "        original_init = dist.init_process_group\n",
        "\n",
        "        try:\n",
        "            # Replace the init function temporarily\n",
        "            dist.init_process_group = dummy_init\n",
        "\n",
        "            # Create the pipe instance\n",
        "            self.pipe = SkyReelsVideoSingleGpuInfer(\n",
        "                task_type=task_type,\n",
        "                model_id=model_id,\n",
        "                quant_model=quant_model,\n",
        "                local_rank=0,\n",
        "                world_size=1,\n",
        "                is_offload=True,\n",
        "                offload_config=offload_config,\n",
        "                enable_cfg_parallel=False\n",
        "            )\n",
        "        finally:\n",
        "            # Restore the original function\n",
        "            dist.init_process_group = original_init\n",
        "\n",
        "    def inference(self, kwargs):\n",
        "        if not isinstance(kwargs, dict):\n",
        "            raise ValueError(\"kwargs must be a dictionary\")\n",
        "\n",
        "        print(f\"Inference kwargs: {kwargs}\")  # Debug print\n",
        "\n",
        "        if \"seed\" in kwargs:\n",
        "            kwargs[\"generator\"] = torch.Generator(\"cuda\").manual_seed(kwargs[\"seed\"])\n",
        "            del kwargs[\"seed\"]\n",
        "\n",
        "        return self.pipe.pipe(**kwargs).frames[0]\n",
        "\n",
        "def count_tokens(text):\n",
        "    \"\"\"Count the number of tokens in a text string\"\"\"\n",
        "    return len(tokenizer.encode(text))\n",
        "\n",
        "def truncate_to_token_limit(text, limit):\n",
        "    \"\"\"Truncate text to stay within token limit while trying to maintain coherent phrases\"\"\"\n",
        "    tokens = tokenizer.encode(text)\n",
        "    if len(tokens) <= limit:\n",
        "        return text\n",
        "\n",
        "    # Decode truncated tokens and try to find a good breaking point\n",
        "    truncated_text = tokenizer.decode(tokens[:limit])\n",
        "\n",
        "    # Try to break at the last sentence\n",
        "    last_period = truncated_text.rfind('.')\n",
        "    if last_period > len(truncated_text) * 0.7:  # Only break at sentence if it's in the latter part\n",
        "        return truncated_text[:last_period + 1]\n",
        "\n",
        "    # Try to break at the last comma\n",
        "    last_comma = truncated_text.rfind(',')\n",
        "    if last_comma > len(truncated_text) * 0.7:\n",
        "        return truncated_text[:last_comma + 1]\n",
        "\n",
        "    # If no good breaking point, just return the truncated text\n",
        "    return truncated_text\n",
        "\n",
        "def enhance_prompt(prompt, negative_prompt):\n",
        "    \"\"\"\n",
        "    Use Claude to enhance both the positive and negative prompts,\n",
        "    ensuring responses don't exceed 77 tokens each\n",
        "    \"\"\"\n",
        "    system = \"\"\"You are an expert at creating prompts for text-to-video AI models.\n",
        "    Given an input prompt and negative prompt, enhance them to create better quality videos.\n",
        "    Return only a JSON object with two keys: 'prompt' and 'negative_prompt'.\n",
        "    Each prompt MUST be 77 tokens or less.\n",
        "    Focus on adding cinematic details and quality specifications.\n",
        "    Do not explain or add any other text.\"\"\"\n",
        "\n",
        "    user_message = f\"\"\"Original prompt: {prompt}\n",
        "    Original negative prompt: {negative_prompt}\n",
        "\n",
        "    Please enhance both prompts while keeping each under 77 tokens.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = claude.messages.create(\n",
        "            model=\"claude-3-5-sonnet-20241022\",\n",
        "            max_tokens=1000,\n",
        "            temperature=0.7,\n",
        "            system=system,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": user_message\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Extract JSON from response\n",
        "        enhanced = json.loads(response.content[0].text)\n",
        "\n",
        "        # Ensure prompts are within token limit\n",
        "        enhanced_prompt = truncate_to_token_limit(enhanced['prompt'], 77)\n",
        "        enhanced_negative = truncate_to_token_limit(enhanced['negative_prompt'], 77)\n",
        "\n",
        "        # Double check token counts\n",
        "        prompt_tokens = count_tokens(enhanced_prompt)\n",
        "        negative_tokens = count_tokens(enhanced_negative)\n",
        "\n",
        "        print(f\"Enhanced prompt tokens: {prompt_tokens}\")\n",
        "        print(f\"Enhanced negative prompt tokens: {negative_tokens}\")\n",
        "\n",
        "        return enhanced_prompt, enhanced_negative\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error enhancing prompt: {str(e)}\")\n",
        "        # Ensure original prompts are within token limit\n",
        "        safe_prompt = truncate_to_token_limit(prompt, 77)\n",
        "        safe_negative = truncate_to_token_limit(negative_prompt, 77)\n",
        "        return safe_prompt, safe_negative\n",
        "\n",
        "def round_to_multiple_of_16(n):\n",
        "    \"\"\"Round a number to the nearest multiple of 16.\"\"\"\n",
        "    return ((n + 8) // 16) * 16\n",
        "\n",
        "def update_prompts(prompt, negative_prompt):\n",
        "    \"\"\"Callback function for the enhance button\"\"\"\n",
        "    try:\n",
        "        enhanced_prompt, enhanced_negative = enhance_prompt(prompt, negative_prompt)\n",
        "        return enhanced_prompt, enhanced_negative\n",
        "    except Exception as e:\n",
        "        print(f\"Error in update_prompts: {str(e)}\")\n",
        "        return prompt, negative_prompt\n",
        "\n",
        "def generate_video(prompt, seed, negative_prompt, width, height, num_frames,\n",
        "                   num_inference_steps, guidance_scale, embedded_guidance_scale, image=None):\n",
        "    global task_type, predictor\n",
        "\n",
        "    # Debug prints\n",
        "    print(f\"Starting video generation with parameters:\")\n",
        "    print(f\"- Image type: {type(image)}\")\n",
        "    print(f\"- Prompt: {prompt}\")\n",
        "    print(f\"- Original dimensions: {width}x{height}\")\n",
        "\n",
        "    # Round dimensions to nearest multiple of 16\n",
        "    width = round_to_multiple_of_16(int(width))\n",
        "    height = round_to_multiple_of_16(int(height))\n",
        "    print(f\"- Adjusted dimensions: {width}x{height}\")\n",
        "\n",
        "    print(f\"- Frames: {num_frames}\")\n",
        "    print(f\"- Steps: {num_inference_steps}\")\n",
        "    print(f\"- Seed: {seed}\")\n",
        "\n",
        "    # Handle random seed\n",
        "    if seed == -1:\n",
        "        random.seed(time.time())\n",
        "        seed = int(random.randrange(4294967294))\n",
        "\n",
        "    # Build kwargs dictionary\n",
        "    kwargs = {\n",
        "        \"prompt\": prompt,\n",
        "        \"height\": height,\n",
        "        \"width\": width,\n",
        "        \"num_frames\": int(num_frames),\n",
        "        \"num_inference_steps\": int(num_inference_steps),\n",
        "        \"seed\": seed,\n",
        "        \"guidance_scale\": float(guidance_scale),\n",
        "        \"embedded_guidance_scale\": float(embedded_guidance_scale),\n",
        "        \"negative_prompt\": negative_prompt,\n",
        "        \"cfg_for\": False\n",
        "    }\n",
        "\n",
        "    # Handle image for i2v mode\n",
        "    if task_type == \"i2v\":\n",
        "        if image is None:\n",
        "            raise ValueError(\"Image is required for image-to-video mode\")\n",
        "\n",
        "        if isinstance(image, str):\n",
        "            try:\n",
        "                image = Image.open(image)\n",
        "                aspect_ratio = image.width / image.height\n",
        "                new_width = width\n",
        "                new_height = int(width / aspect_ratio)\n",
        "                if new_height > height:\n",
        "                    new_height = height\n",
        "                    new_width = int(height * aspect_ratio)\n",
        "                new_width = round_to_multiple_of_16(new_width)\n",
        "                new_height = round_to_multiple_of_16(new_height)\n",
        "                image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"Error processing image: {str(e)}\")\n",
        "\n",
        "        kwargs[\"image\"] = image\n",
        "\n",
        "    print(f\"Final generation parameters: {kwargs}\")\n",
        "\n",
        "    try:\n",
        "        # Generate the video\n",
        "        output = predictor.inference(kwargs)\n",
        "\n",
        "        # Save the video\n",
        "        save_dir = f\"./result/{task_type}\"\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        base_filename = f\"{prompt[:50].replace(' ', '_').replace('/','')}_{width}x{height}_{num_frames}fr_{seed}\"\n",
        "        video_out_file = f\"{save_dir}/{base_filename}.mp4\"\n",
        "\n",
        "        print(f\"Saving video to: {video_out_file}\")\n",
        "        export_to_video(output, video_out_file, fps=24)\n",
        "\n",
        "        return video_out_file, kwargs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during video generation: {str(e)}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        raise ValueError(f\"Video generation failed: {str(e)}\")\n",
        "\n",
        "def init_predictor(task_type: str):\n",
        "    global predictor\n",
        "    try:\n",
        "        # Set CUDA device first\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.init()\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.set_device(0)\n",
        "\n",
        "        # Initialize process group first\n",
        "        init_process_group()\n",
        "\n",
        "        model_id = \"Skywork/SkyReels-V1-Hunyuan-I2V\" if task_type == \"i2v\" else \"Skywork/SkyReels-V1-Hunyuan-T2V\"\n",
        "\n",
        "        predictor = SimpleSkyReelsInfer(\n",
        "            task_type=TaskType.I2V if task_type == \"i2v\" else TaskType.T2V,\n",
        "            model_id=model_id,\n",
        "            quant_model=True\n",
        "        )\n",
        "        print(\"Predictor initialized successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Initialization error: {str(e)}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        raise\n",
        "\n",
        "def create_gradio_interface(task_type):\n",
        "    \"\"\"Create an enhanced Gradio interface with prompt enhancement.\"\"\"\n",
        "\n",
        "    css = \"\"\"\n",
        "        .container { margin: 15px; }\n",
        "        .output-panel { margin-top: 20px; }\n",
        "        .control-panel { padding: 10px; border: 1px solid #ccc; border-radius: 8px; }\n",
        "        .enhance-button { margin: 10px 0; }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(css=css) as demo:\n",
        "        gr.Markdown(\"# SkyReels Video Generation\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### Input Controls\")\n",
        "                with gr.Group(elem_classes=\"control-panel\"):\n",
        "                    # Image upload for i2v mode\n",
        "                    if task_type == \"i2v\":\n",
        "                        image = gr.Image(\n",
        "                            label=\"Upload Image\",\n",
        "                            type=\"filepath\",\n",
        "                            # tool=\"editor\",\n",
        "                            elem_id=\"input-image\"\n",
        "                        )\n",
        "\n",
        "                    # Prompt inputs with enhancement\n",
        "                    with gr.Group():\n",
        "                        prompt = gr.Textbox(\n",
        "                            label=\"Input Prompt\",\n",
        "                            placeholder=\"Describe what you want to generate...\",\n",
        "                            lines=3\n",
        "                        )\n",
        "                        negative_prompt = gr.Textbox(\n",
        "                            label=\"Negative Prompt\",\n",
        "                            value=\"Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion\",\n",
        "                            lines=2\n",
        "                        )\n",
        "                        enhance_button = gr.Button(\n",
        "                            \"âœ¨ Enhance Prompts with AI\",\n",
        "                            elem_classes=\"enhance-button\"\n",
        "                        )\n",
        "\n",
        "                    with gr.Accordion(\"Advanced Settings\", open=False):\n",
        "                        seed = gr.Number(\n",
        "                            label=\"Random Seed (-1 for random)\",\n",
        "                            value=-1,\n",
        "                            minimum=-1,\n",
        "                            precision=0\n",
        "                        )\n",
        "                        width = gr.Slider(\n",
        "                            label=\"Width\",\n",
        "                            minimum=128,\n",
        "                            maximum=1024,\n",
        "                            value=512,\n",
        "                            step=16\n",
        "                        )\n",
        "                        height = gr.Slider(\n",
        "                            label=\"Height\",\n",
        "                            minimum=128,\n",
        "                            maximum=1024,\n",
        "                            value=512,\n",
        "                            step=16\n",
        "                        )\n",
        "                        num_frames = gr.Slider(\n",
        "                            label=\"Number of Frames\",\n",
        "                            minimum=16,\n",
        "                            maximum=128,\n",
        "                            value=97,\n",
        "                            step=1\n",
        "                        )\n",
        "                        num_inference_steps = gr.Slider(\n",
        "                            label=\"Inference Steps\",\n",
        "                            minimum=1,\n",
        "                            maximum=100,\n",
        "                            value=30,\n",
        "                            step=1\n",
        "                        )\n",
        "                        guidance_scale = gr.Slider(\n",
        "                            label=\"Guidance Scale\",\n",
        "                            minimum=1,\n",
        "                            maximum=20,\n",
        "                            value=6.0,\n",
        "                            step=0.5\n",
        "                        )\n",
        "                        embedded_guidance_scale = gr.Slider(\n",
        "                            label=\"Embedded Guidance Scale\",\n",
        "                            minimum=0.1,\n",
        "                            maximum=5.0,\n",
        "                            value=1.0,\n",
        "                            step=0.1\n",
        "                        )\n",
        "\n",
        "                    generate_button = gr.Button(\n",
        "                        \"Generate Video\",\n",
        "                        variant=\"primary\",\n",
        "                        size=\"lg\"\n",
        "                    )\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### Output\")\n",
        "                with gr.Group(elem_classes=\"output-panel\"):\n",
        "                    output_video = gr.Video(label=\"Generated Video\")\n",
        "                    output_params = gr.JSON(label=\"Generation Parameters\")\n",
        "\n",
        "        # Set up event handlers\n",
        "        enhance_button.click(\n",
        "            fn=update_prompts,\n",
        "            inputs=[prompt, negative_prompt],\n",
        "            outputs=[prompt, negative_prompt]\n",
        "        )\n",
        "\n",
        "        input_components = [\n",
        "            prompt,\n",
        "            seed,\n",
        "            negative_prompt,\n",
        "            width,\n",
        "            height,\n",
        "            num_frames,\n",
        "            num_inference_steps,\n",
        "            guidance_scale,\n",
        "            embedded_guidance_scale\n",
        "        ]\n",
        "\n",
        "        if task_type == \"i2v\":\n",
        "            input_components.append(image)\n",
        "\n",
        "        generate_button.click(\n",
        "            fn=generate_video,\n",
        "            inputs=input_components,\n",
        "            outputs=[output_video, output_params]\n",
        "        )\n",
        "\n",
        "        # Add help section\n",
        "        with gr.Accordion(\"Help\", open=False):\n",
        "            gr.Markdown(\"\"\"\n",
        "                ### Tips for best results:\n",
        "                - Use the \"Enhance Prompts\" button to improve your prompts with AI\n",
        "                - Use clear, descriptive prompts\n",
        "                - Experiment with different guidance scales\n",
        "                - Try different seeds for variation\n",
        "                - Adjust frame count based on your needs\n",
        "\n",
        "                ### Common issues:\n",
        "                - If generation fails, try reducing the resolution\n",
        "                - High guidance scales may produce stronger but potentially less stable results\n",
        "                - Larger frame counts will take longer to generate\n",
        "            \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set environmental variables\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "    # Print system info\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
        "# @markdown For Text-To-Video = t2v or For  Image-To-Video = i2v\n",
        "    task_type = \"t2v\" # @param [\"i2v\",\"t2v\"]\n",
        "    print(\"Starting initialization...\")\n",
        "    init_predictor(task_type)\n",
        "\n",
        "    # Create and launch the Gradio interface\n",
        "    demo = create_gradio_interface(task_type)\n",
        "    demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "DvNnyOY7IHgr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}